<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>CS180 Project 3A: IMAGE WARPING and MOSAICING</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 40px;
      line-height: 1.6;
    }
    h1, h2, h3 {
      margin-top: 40px;
    }
    h2 {
      border-bottom: 2px solid #ccc;
      padding-bottom: 5px;
    }
    .section {
      margin-bottom: 50px;
    }
    .image-row {
      display: flex;
      justify-content: center;
      gap: 20px;
      margin-top: 20px;
      flex-wrap: wrap;
    }
    .image-container {
      text-align: center;
      max-width: 30%;
    }
    .image-container img {
      width: 100%;
      height: auto;
      border: 1px solid #ccc;
    }
    .full-width-image {
      width: 60%;
      max-width: 70%;
      margin: 20px 0;
      text-align: center;
    }

    .full-width-image img {
      width: 100%;      /* scale to fit screen width */
      height: auto;     /* maintain aspect ratio */
      display: block;
      margin: 0 auto;
      border: 1px solid #ccc;
      max-width: 100%;  /* ensure it doesn‚Äôt overflow container */
    }

    .full-width-image .grid-caption {
      margin-top: 8px;
      font-size: 0.9em;
      color: #555;
    }

    .caption {
      margin-top: 8px;
      font-size: 0.9em;
      color: #555;
    }
    .grid {
      display: grid;
      grid-template-columns: repeat(3, 200px); /* fixed column width */
      gap: 20px;
      margin: 20px auto;         /* center the grid */
      justify-content: center;   /* center grid items */
    }

    .grid img {
      width: 100%;      /* force image to fit column */
      max-width: 200px; /* don‚Äôt let it stretch bigger */
      height: auto;     /* keep aspect ratio */
      object-fit: contain; /* shrink properly inside cell */
      border: 1px solid #ccc;
    }
    .grid-caption {
      text-align: center;
      margin-top: 6px;
      font-size: 0.9em;
      color: #555;
    }
    code {
      background-color: #efe0bf;
      padding: 2px 4px;
      border-radius: 4px;
      font-size: 0.95em;
    }
    .stack-image {
      display: block;
      margin: 20px auto;   /* center each one vertically stacked */
      max-width: 800px;    /* or whatever width you want */
      width: 100%;         /* scale down responsively */
      height: auto;        /* keep aspect ratio */
      border: 1px solid #ccc;
    }
    .stack-caption {
      text-align: center;
      margin-top: 8px;
      font-size: 0.9em;
      color: #555;
    }

  </style>
</head>
<body>

  <h1>CS180 Project 3A: IMAGE WARPING and MOSAICING</h1>

  <!-- Part 1 -->
  <div class="section">

    <h3>A.1 - Shoot the Pictures</h3>
    <p>
    
    </p>

    <div class="full-width-image">
      <img src="./out/a1_rotate.png" alt="Orange Laplacian stack">
      <div class="grid-caption">Rotating Images With Same Center of Projection</div>
    </div>
    <div class="full-width-image">
      <img src="./out/a1_rotate2.png" alt="Orange Laplacian stack">
      <div class="grid-caption">Rotating Images With Same Center of Projection</div>
    </div>
    <div class="full-width-image">
      <img src="./out/a1_rotate3.png" alt="Orange Laplacian stack">
      <div class="grid-caption">Rotating Images With Same Center of Projection</div>
    </div>

    <h3>A.2 - Recovering Homographies</h3>


    <p>
      I implemented the <code>compute_H</code> constructs a linear system 
ùê¥
‚Ñé
=
0
Ah=0 from point correspondences between im1_pts and im2_pts, where each point pair contributes two equations relating coordinates before and after transformation.
    </p>

    <div class="full-width-image">
        <img src="./out/compute_H.png" alt="Orange Laplacian stack">
        <div class="grid-caption">Rotating Images With Same Center of Projection</div>
    </div>
    <div class="full-width-image">
        <img src="./out/h_coffee.png" alt="Orange Laplacian stack">
        <div class="grid-caption">Rotating Images With Same Center of Projection</div>
    </div>
    <div class="full-width-image">
        <img src="./out/h_laundry.png" alt="Orange Laplacian stack">
        <div class="grid-caption">Rotating Images With Same Center of Projection</div>
    </div>
    <div class="full-width-image">
        <img src="./out/h_sea.png" alt="Orange Laplacian stack">
        <div class="grid-caption">Rotating Images With Same Center of Projection</div>
    </div>
    <div class="full-width-image">
        <img src="./out/h_stairs.png" alt="Orange Laplacian stack">
        <div class="grid-caption">Rotating Images With Same Center of Projection</div>
    </div>

    
    
    
    <h3>A.3 Image Warping</h3>
    <p>
      To begin my process, I took 2 images of a Louis Armstrong poster from different angles, the forward facing one being the reference image, and the tilted/angled one being the target image to warp.
    </p>

    <p>
      I then selected 8 points in the reference image that correspond to the same points in the target image. Using these points, I computed the homography matrix that maps points from the target image to the reference image with my <code>compute_H</code> function.
    </p>
    
    <div class="full-width-image">
        <img src="./out/louis_warp_bil.png" alt="Orange Laplacian stack">
        <div class="grid-caption">Warping with Bilinear Interpolation</div>
    </div>
    <div class="image-row">
      <div class="image-container">
        <img src="./out/imwarped_bil_poster.jpg" alt="Oraple">
        <div class="caption">Warping with Bilinear Interpolation</div>
      </div>
      <div class="image-container">
        <img src="./out/zoom_bil.png" alt="Custom blend 1">
        <div class="caption">Zoomed in with Bilinear Interpolation</div>
      </div>
    </div>
    <h4>
    More Warping with Bilinear Interpolation 
  </h4>
  <div class="full-width-image">
        <img src="./out/warped_outlet_bil_full.png" alt="Orange Laplacian stack">
        <div class="grid-caption">Warping with Nearest Neighbor Interpolation</div>
    </div>
    <div class="image-row">
      <div class="image-container">
        <img src="./out/warped_outlet_bil.jpg" alt="Oraple">
        <div class="caption">Warping with Nearest Neighbor Interpolation</div>
      </div>
      <!-- <div class="image-container">
        <img src="./out/zoom_nn.png" alt="Custom blend 1">
        <div class="caption">Zoomed in with Nearest Neighbor Interpolation</div>
      </div> -->
    </div>
  </div>
    <p>
      With nearest neighbor interpolation, the images achieved a smoother blending between different pixels than with bilinear interpolation. While we see in bilinear interpolation that there appears to be almost a grid pattern present between every few pixels when covering the "holes" that emerge due to the homography transformation, nearest neighbor interpolation is able to fill in these gaps with the color of the nearest pixel, resulting in a more cohesive image.
    </p>
    <div class="full-width-image">
        <img src="./out/lous_warp_nn.png" alt="Orange Laplacian stack">
        <div class="grid-caption">Warping with Nearest Neighbor Interpolation</div>
    </div>
    <div class="image-row">
      <div class="image-container">
        <img src="./out/imwarped_nn_poster.jpg" alt="Oraple">
        <div class="caption">Warping with Nearest Neighbor Interpolation</div>
      </div>
      <div class="image-container">
        <img src="./out/zoom_nn.png" alt="Custom blend 1">
        <div class="caption">Zoomed in with Nearest Neighbor Interpolation</div>
      </div>
    </div>
  </div>
  <h4>
    More Warping with Nearest Neighbor 
  </h4>
  <div class="full-width-image">
        <img src="./out/warped_outlet_nn_full.png" alt="Orange Laplacian stack">
        <div class="grid-caption">Warping with Nearest Neighbor Interpolation</div>
    </div>
    <div class="image-row">
      <div class="image-container">
        <img src="./out/warped_outlet_nn.jpg" alt="Oraple">
        <div class="caption">Warping with Nearest Neighbor Interpolation</div>
      </div>
      <!-- <div class="image-container">
        <img src="./out/zoom_nn.png" alt="Custom blend 1">
        <div class="caption">Zoomed in with Nearest Neighbor Interpolation</div>
      </div> -->
    </div>
  </div>
    <!-- <div class="grid"> -->
  
    <!-- </div> -->
    

    <h3>A.4 - Mosaics Results</h3>
    <p>
      This part proved to be the most challenging part of the project, as I had trouble getting a mask to work that results in a smooth blending and no black edges around it. 
      I tried a few different approaches, including using a binary mask that is 1 where the warped image has valid pixels and 0 elsewhere, and then using this mask to blend the warped image with the reference image. However, this approach resulted in visible seams between the two images since it was a mask around only the pixels of the warped image
    </p>

    <p>
      I then realized that using a distance transform for the mask is a much better approach. I was able to now reduce the mask to only the overlapping parts of the image, and then use a distance transform to create a smooth transition between both images
    </p>


    <div class="full-width-image">
        <img src="./out/coffee_blend.jpg" alt="Orange Laplacian stack">
        <div class="grid-caption">Berkeley Espresso</div>
    </div>
    <div class="full-width-image">
        <img src="./out/laundry_blend.jpg" alt="Orange Laplacian stack">
        <div class="grid-caption">Laundry Room</div>
    </div>

    <p>
      However, the area around the blending of both images still appears blurry in the first 2 mosaics, but I was able to get a much better transition on the last image. Perhaps this is due to the wrong homography matrix, or not enough overlapping points between the two images, I'm looking to explore this 
    </p>
    <div class="full-width-image">
        <img src="./out/sea_blend.jpg" alt="Orange Laplacian stack">
        <div class="grid-caption">Rainy day in Seattle</div>
    </div>
  

  
  <h1>CS180 Project 3B: Feature Matching for Autostitching</h1>

  <!-- Part 1 -->
  <div class="section">

    <h3>B.1 - Harris Corner Detection</h3>
    <p>
      When I first ran Harris Corner Detection on this image, I was worried that a lot of the selected corners were not in fact signifcant corners, and it seemed to span across the entire image. However, I still noticed that many of the corners did lie around the main subject of the image, which would help in future feature detection.
    </p>

    <p>
      I also experimented with different values for min_distance, and I found that increasing this value helped to spread out the corners evenly across the imge, but it did not get rid of the less significant ones in the background. It seemed that the only way to filter the insiginifcant corners successfully was with ANMS.
    </p>

    <div class="full-width-image">
      <img src="./out2/harris1.png" alt="Orange Laplacian stack">
      <div class="grid-caption">Harris Corner Detection</div>
    </div>
    <h4>  
      Harris Detection with ANMS
    </h4>
    <p>
      After implementing ANMS, I was able to significantly reduce the number of detected corners while maintaining a good distribution across the image. This helped in selecting the most prominent features and removed most of the features in the background where the fog and sky is.
    </p>
    <div class="full-width-image">
      <img src="./out2/anms1.png" alt="Orange Laplacian stack">
      <div class="grid-caption"></div>
    </div>
    <div class="full-width-image">
      <img src="./out2/anms2.png" alt="Orange Laplacian stack">
      <div class="grid-caption"></div>
    </div>
    <p>
      Additionally, I was happy to see that a lot of the filtered corners across both images seemed to concentrate around the same areas, which would also help in future feature matching because it limits the set of features that could be the same in both images to only the similar ones in location in both images.
    </p>

    <h3>B.2 - Feature Descriptor Extactor</h3>


    <p>
To do this, I iterated though each of the filtered corners from ANMS and extracted a 40x40 pixel patch around each corner. Then, I resized the patch using cv2.resize to an 8x8 pixel patch, which I then flattened into a 64-dimensional vector. Finally, I normalized the vector to have zero mean and unit variance to ensure that the descriptors were invariant to lighting changes.
    </p>
    <p>
      I repeated this process for both image1 and image2.
    </p>

    <div class="full-width-image">
        <img src="./out2/features1.png" alt="Orange Laplacian stack">
        <div class="grid-caption">Extracted Features from Image1</div>
    </div>
    <div class="full-width-image">
        <img src="./out2/features2.png" alt="Orange Laplacian stack">
        <div class="grid-caption">Extracted Features from Image2</div>
    </div>
    

    
    
    
    <h3>B.3 - Feature Matching</h3>
    <p>
      For the feature matching, I iterated through each feature descriptor and identified its 2 nearest neighbors in the other image using Lowe's ratio test, and was able to set a threshold of 0.7 for this image that filtered out most bad matches. Then I displayed the matches across both images, and adjusted the parameters to get a good number of accurate matches.
    </p>
    
    <div class="full-width-image">
        <img src="./out2/matching.png" alt="Orange Laplacian stack">
        <div class="grid-caption">Warping with Bilinear Interpolation</div>
    </div>
  </div>
    <h3>B.4 - Mosaics Results</h3>
    <p>
      Using my mask and mosiac blending implementation from part 3A, I was able to use the created homography matrix to warp and blend my two images in Mountain Tamalpais, and it seemed that everything worked out, since the only thing I really changed was the values of the homography matrix H. 
     </p>

     <p>
      I then used the same process on another set of images of Kresge Library, and they also seemed to blend well using the same threshold, c, and min_distance values as the Mountain Tamalpais image.
     </p>

    <p>
      
    </p>


    <div class="full-width-image">
        <img src="./out2/kresge_blend.jpg" alt="Orange Laplacian stack">
        <div class="grid-caption">Kresge Library</div>
    </div>

    <p>
      However, when I tried to use the same process on the coffee shop image sets I used in part A of the project, the automatic feature matching did not work well, and the images were misaligned significantly. I tried to lower the threshold min_distance values, which reduced the blurring in the blending, but it still did not compare to the manual stitching mosiac result from part A. I believe that the automatic feature matching simply did not find good enough corresponding points and features perhaps due to how busy the image was, while with the manual selection, I was able to pick out the best points to align the images well.
    </p>
    <div class="full-width-image">
        <img src="./out2/coffee_blend.jpg" alt="Orange Laplacian stack">
        <div class="grid-caption">Coffee Shop Blend with Automatic Stitching</div>
    </div>
    <div class="full-width-image">
        <img src="./out2/coffee_blend_manual.jpg" alt="Orange Laplacian stack">
        <div class="grid-caption">Coffee Shop Blend with MANUAL Stitching</div>
    </div>
  
</body>
</html>

