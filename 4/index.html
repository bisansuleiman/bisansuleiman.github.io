<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>CS180 Project 4: Neural Radiance Field</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 40px;
      line-height: 1.6;
    }
    h1, h2, h3 {
      margin-top: 40px;
    }
    h2 {
      border-bottom: 2px solid #ccc;
      padding-bottom: 5px;
    }
    .section {
      margin-bottom: 50px;
    }
    .image-row {
      display: flex;
      justify-content: center;
      gap: 20px;
      margin-top: 20px;
      flex-wrap: wrap;
    }
    .image-container {
      text-align: center;
      max-width: 30%;
    }
    .image-container img {
      width: 100%;
      height: auto;
      border: 1px solid #ccc;
    }
    .full-width-image {
      width: 60%;
      max-width: 70%;
      margin: 20px 0;
      text-align: center;
    }

    .full-width-image img {
      width: 100%;      /* scale to fit screen width */
      height: auto;     /* maintain aspect ratio */
      display: block;
      margin: 0 auto;
      border: 1px solid #ccc;
      max-width: 100%;  /* ensure it doesn’t overflow container */
    }

    .full-width-image .grid-caption {
      margin-top: 8px;
      font-size: 0.9em;
      color: #555;
    }

    .caption {
      margin-top: 8px;
      font-size: 0.9em;
      color: #555;
    }
    .grid {
      display: grid;
      grid-template-columns: repeat(3, 200px); /* fixed column width */
      gap: 20px;
      margin: 20px auto;         /* center the grid */
      justify-content: center;   /* center grid items */
    }

    .grid img {
      width: 100%;      /* force image to fit column */
      max-width: 200px; /* don’t let it stretch bigger */
      height: auto;     /* keep aspect ratio */
      object-fit: contain; /* shrink properly inside cell */
      border: 1px solid #ccc;
    }
    .grid-caption {
      text-align: center;
      margin-top: 6px;
      font-size: 0.9em;
      color: #555;
    }
    code {
      background-color: #efe0bf;
      padding: 2px 4px;
      border-radius: 4px;
      font-size: 0.95em;
    }
    .stack-image {
      display: block;
      margin: 20px auto;   /* center each one vertically stacked */
      max-width: 800px;    /* or whatever width you want */
      width: 100%;         /* scale down responsively */
      height: auto;        /* keep aspect ratio */
      border: 1px solid #ccc;
    }
    .stack-caption {
      text-align: center;
      margin-top: 8px;
      font-size: 0.9em;
      color: #555;
    }

  </style>
</head>
<body>

  <h1>CS180 Project 4: Neural Radiance Field</h1>

  <!-- Part 1 -->
  <div class="section">

    <h3>Part 0 - Camera Calibration and 3D Scanning</h3>

    <p>
      Screenshots of camera frustums visualization in Viser
    </p>

    <div class="full-width-image">
      <img src="./out/render4.png" alt="Orange Laplacian stack">
      <div class="grid-caption">Camera Frustum 1</div>
    </div>
    <div class="full-width-image">
      <img src="./out/render5.png" alt="Orange Laplacian stack">
      <div class="grid-caption">Camera Frustum 2</div>
    </div>

    <h3>Part 1 - Fit a Neural Field to a 2D Image</h3>


    <h4>
        Model architecture and training report:
    </h4>
    <p>
        For this part of the project, I implemented a simple MLP with 4 hidden layers of 128 neurons each, using ReLU activations. I also included positional encoding with a maximum frequency of 10 to help the model capture high-frequency details in the image. I trained the model using the Adam optimizer with a learning rate of 0.001 and a batch size of 1024 for a total of 1000 iterations. The loss function used was Mean Squared Error (MSE) between the predicted pixel colors and the ground truth pixel colors.
    </p>
    <h4>
      Test image progression
    </h4>

    <div class="full-width-image">
        <img src="./image1_iter_20.png" alt="Orange Laplacian stack">
        <div class="grid-caption">Predicted test image at iteration 20</div>
    </div>
    <div class="full-width-image">
        <img src="./image1_iter_40.png" alt="Orange Laplacian stack">
        <div class="grid-caption">Predicted test image at iteration 40</div>
    </div>
    <div class="full-width-image">
        <img src="./image1_iter_100.png" alt="Orange Laplacian stack">
        <div class="grid-caption">Predicted test image at iteration 100</div>
    </div>
    <div class="full-width-image">
        <img src="./image1_iter_300.png" alt="Orange Laplacian stack">
        <div class="grid-caption">Predicted test image at iteration 300</div>
    </div>
     <div class="full-width-image">
        <img src="./image1_iter_900.png" alt="Orange Laplacian stack">
        <div class="grid-caption">Predicted test image at iteration 900</div>
    </div>


    <h4>
      Own image progression
    </h4>

    <div class="full-width-image">
        <img src="./out/image2_iter_20.png" alt="Orange Laplacian stack">
        <div class="grid-caption">Predicted test image at iteration 20</div>
    </div>
    <div class="full-width-image">
        <img src="./out/image2_iter_40.png" alt="Orange Laplacian stack">
        <div class="grid-caption">Predicted test image at iteration 40</div>
    </div>
    <div class="full-width-image">
        <img src="./out/image2_iter_100.png" alt="Orange Laplacian stack">
        <div class="grid-caption">Predicted test image at iteration 100</div>
    </div>
    <div class="full-width-image">
        <img src="./out/image2_iter_300.png" alt="Orange Laplacian stack">
        <div class="grid-caption">Predicted test image at iteration 300</div>
    </div>
    <div class="full-width-image">
        <img src="./out/hw-lf/image2_iter_999.png" alt="Orange Laplacian stack">
        <div class="grid-caption">Predicted test image at iteration 1000</div>
    </div>
    <h4>
      2x2 Grid - max positional encoding frequency and width combos 
    </h4>
    
    <div class="full-width-image">
        <img src="./out/lw-lf/image2_iter_999.png" alt="Orange Laplacian stack">
        <div class="grid-caption">Low-Width, Low Frequency: at iteration 1000</div>
    </div>
        <div class="full-width-image">
        <img src="./out/hw-lf/image2_iter_999.png" alt="Orange Laplacian stack">
        <div class="grid-caption">High-Width, Low Frequency: at iteration 1000</div>
    </div>
    <div class="full-width-image">
        <img src="./out/hw-hf/image2_iter_999.png" alt="Orange Laplacian stack">
        <div class="grid-caption">High-Width, High Frequency: at iteration 1000</div>
    </div>
        <div class="full-width-image">
        <img src="./out/lw-hf/image2_iter_999.png" alt="Orange Laplacian stack">
        <div class="grid-caption">Low-Width, High Frequency: at iteration 1000</div>
    </div>
    

    <h4>
        PSNR curve for training my own image 
    </h4>
    <div class="full-width-image">
        <img src="./out/hw-lf/psnr_curve_img2.png" alt="PSNR Curve">
        <div class="grid-caption">PSNR Curve for Image2</div>
    </div>
    <h3>Part 2</h3>
    <h4>
        2.1 - Create Rays from Camera
    </h4>
    <p>
        <code>pixel_to_camera</code> implementation:I take in 2D pixel coordinates (uv), the camera intrinsic matrix K, and a depth value s. I then transform these pixel coordinates into 3D camera coordinates by inverting the projection equation s[u,v] = K * x_c. My implementation handles both single points and batched inputs, automatically expanding scalar depth values across batches when needed.
    </p>

    <p>
        <code>pixel_to_ray</code>  implementation: I convert 2D pixel coordinates into 3D rays in world space. Given pixel locations (uv), intrinsic matrix K, and camera-to-world transformation c2w, I first find a point along the ray at depth s=1 in camera coordinates using pixel_to_camera, then transform it to world coordinates. I extract the ray origin r_o from the camera position (c2w[:3, 3]) and compute the ray direction r_d as the vector from r_o to the transformed point. I normalize only r_d (not r_o) to get unit direction vectors. My implementation handles both single and batched inputs.
    </p>
    <h4>
        2.2 - Sampling
    </h4>
    <p>
        <code>sample_rays</code>: I sample rays from multiple images and points along those rays. First, I randomly select n images and sample num_rays=n//m random pixel coordinates from each image. For each pixel, I add a 0.5 offset to the UV coordinates to align with the image grid, then convert them to rays using pixel_to_ray. Next, I sample points along each ray by creating linearly spaced t values between near and far bounds. During training, I add random perturbations to t within each interval to improve sampling. Finally, I compute 3D point coordinates using the ray equation x = r_o + r_d * t.
    </p>
    <p>
        <code>RaysDataLoader</code>: I pre-compute and efficiently manage rays for an entire dataset. At initialization, I generate all rays for every pixel in every image using pixel_to_ray, storing ray origins, directions, and pixel colors in flattened arrays on the CPU to manage memory. When sampling, I randomly select images and pixel indices, then move only the selected rays to the GPU for training. This approach is more efficient than the first method—instead of computing rays on-the-fly, I pre-compute everything once and use fast indexing for sampling. I also provide sample_along_rays to generate 3D sample points along rays using the equation x = r_o + r_d * t, with optional perturbation during training.
    </p>

    <h4>
        2.3 - Ray visualization
    </h4>
    <div class="full-width-image">
        <img src="./out/2-3/render_new_1.png" alt="Ray Visualization">
        <div class="grid-caption">Ray Visualization for Lego Dataset</div>
    </div>
    <h4>
        2.4 - Neural Radiance Field
    </h4>
    <p>
        <code>NeuralRadianceField</code>: I implement a NeRF model that predicts color and density for 3D points. I use positional encoding on both 3D coordinates (p_freq=10) and view directions (d_freq=4). My architecture follows the original NeRF design with 8 layers: first, I process position encodings through 4 layers, then apply a skip connection by concatenating the original position encoding, followed by 4 more layers. For density prediction, I use a single linear layer with Softplus activation (instead of ReLU) to ensure smooth, positive density values. For color prediction, I concatenate the view direction encoding with the features and pass them through two layers with Sigmoid output to produce RGB values in [0,1]. The key design choice is that density is view-independent while color is view-dependent, allowing the model to capture view-dependent effects like specularities.
    </p>
    <p>
        <code>PositionalEncoding3d</code>: I transform 3D coordinates into high-dimensional encodings using sinusoidal functions. For each input coordinate, I generate frequency bands from 2^0 to 2^(max_freq-1) and apply sine and cosine functions at each frequency. I concatenate the original coordinates with all the encoded values, producing an output dimension of 3 + 3×2×max_freq. This encoding helps the network learn high-frequency details in the scene.
    </p>
    

      <h4>
        2.5 - Volume Rendering
    </h4>
    <p>
        <code>volrend</code>: Given densities (B, N, 1) and colors (B, N, 3) from N samples along B rays, I use the volume rendering equation to accumulate colors weighted by transmittance and opacity. First, I compute the probability of ray termination at each sample as α_i = 1 - exp(-σ_i × δ), where σ_i is density and δ is step_size. Then I calculate cumulative transmittance T_i as the product of (1 - α_j) for all samples j before i, using torch.cumprod and prepending ones to handle the first sample. Finally, I compute the rendered color as the sum of T_i × α_i × c_i across all samples, which represents how much each sample contributes to the final pixel color based on its opacity and how much light reaches it. My output is (B, 3) RGB colors for each ray.
    </p>

    <p>
        Training Script: I train the NeRF model using ray-based sampling and volume rendering. Each iteration, I sample a batch of rays from multiple training images using RaysDataLoader, then sample n_samples=64 points along each ray between near=2.0 and far=6.0 with random perturbations during training. I pass the 3D points and ray directions to the network (letting it handle direction expansion internally), compute rendered colors using volume rendering, and optimize with MSE loss against ground truth pixels. Every 100 iterations, I validate by rendering full images in batches to avoid memory issues, computing PSNR for each validation image and averaging the results. I use batch_size=4096, learning_rate=1e-3, and train for 1000 iterations, visualizing and saving validation results throughout training.
    </p>


    <h4>
        Volume Rendering Lego Dataset
    </h4>
    <p>
        For this section, I struggled to get a good validation result of a PSNR value above 13 at first. I discovered that my <code>volrend</code> function was formatted incorrectly, which was leading to rendering inaccuracies for my ray to pixel values. Then, I was able to raise my PSNR value to around 17, but I still was not able to achieve the needed validation PSNR of 23. After attending office hours and a lot of debugging, we realized that some of the near and far values could be readjusted, and I could increase the batch size from 1000, to 10K. The near and far values also changed to the given values in Ed, and I was able to get to a PSNR value of 23 after around 1000 rotations, and ~25 PSNR after 2000 iterations.

    </p>
    <div class="full-width-image">
        <img src="./out/2-5/psnr_curve.png" alt="Ray Visualization">
        <div class="grid-caption">PSNR curve of Lego Dataset</div>
    </div>
    <div class="full-width-image">
        <video src="./out/2-5/lego_spherical.mp4" alt="Ray Visualization">
        <div class="grid-caption">novel view image of lego dataset</div>
    </div>

    <h4>
        more on the Lego dataset at each step
    </h4>
    <div class="full-width-image">
        <img src="./out/2-5/new/val_iter_200.png" alt="Ray Visualization">
        <div class="grid-caption"></div>
    </div>
  <div class="full-width-image">
        <img src="./out/2-5/new/val_iter_400.png" alt="Ray Visualization">
        <div class="grid-caption"></div>
    </div>
      <div class="full-width-image">
        <img src="./out/2-5/new/val_iter_1100.png" alt="Ray Visualization">
        <div class="grid-caption"></div>
    </div>
      <div class="full-width-image">
        <img src="./out/2-5/new/val_iter_1700.png" alt="Ray Visualization">
        <div class="grid-caption"></div>
    </div>


    
  </div>
    <h3>2.6 - Training with your own data</h3>
    

    <p>
      For this section, I struggled to get a good validation result of a PSNR value above 13 as well at first. Also after fixing the <code>volrend</code> function from 2.5 for the Lego dataset, I had a hard time achieving the same PSNR values for validation as the Lego dataset. After attending office hours I realized that the network is taking much longer to learn my images as my images were much larger (3x the size) than the image dimensions of the lego dataset. Therefore I ended up resizing each image in the dataset to around 300x240 pixels, instead of aroun 1200 originally. This very much improved my PSNR value, and I was able to get to a PSNR of ~20 much faster in the network than earlier. However, my rendered images were not that clear, and I still struggled to obtain a good novel camera view towards the end. Please be nice with the grading of this section, I'm turning this late already, and it was quite a hard project. I really tried ...
      </p>
  

         <p>
            I then ended up running this script with a near=0.02 and a far=1.0, and I was able to get pretty good validation results, almost of ~20 PSNR after 10K iterations. However, again, this did not translate well in my novel view
      </p>

    </div>
        <div class="full-width-image">
        <img src="./out/2-6/psnr_curve.png" alt="Ray Visualization">
        <div class="grid-caption">Loss training curve</div>
    </div>
    </div>
        <div class="full-width-image">
        <img src="./out/2-6/val_img0_iter_5300.png" alt="Ray Visualization">
        <div class="grid-caption">validation rendered result at iter=5300</div>
    </div>
      <div class="full-width-image">
        <img src="./out/2-6/val_img0_iter_7700.png" alt="Ray Visualization">
        <div class="grid-caption">validation rendered resul at iter=7700</div>
    </div>
         <div class="full-width-image">
        <img src="./out/2-6/val_img0_iter_9200.png" alt="Ray Visualization">
        <div class="grid-caption">validation rendered result at iter=9200</div>
    </div>
    </div>
         <div class="full-width-image">
        <img src="./out/2-6/val_img0_iter_9700.png" alt="Ray Visualization">
        <div class="grid-caption">validation rendered result at iter=9700</div>
    </div>

         <div class="full-width-image">
        <img src="./out/2-6/rotation.gif" alt="Ray Visualization">
        <div class="grid-caption">novel view image of lego dataset</div>
    </div>
</body>
</html>
